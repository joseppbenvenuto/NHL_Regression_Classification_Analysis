{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Functions\n",
    "\n",
    "### Description\n",
    "\n",
    "Below are the different functions used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "\n",
    "from scipy import stats\n",
    "import math as ma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Test Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns regression metrics\n",
    "def regression_test_metrics(y, y_pred):\n",
    "\n",
    "    r2 = round(r2_score(y, y_pred),2)\n",
    "    mae = round(mean_absolute_error(y, y_pred),2)\n",
    "    mse = round(mean_squared_error(y, y_pred),2)\n",
    "    rmse = round(ma.sqrt(mse),2)\n",
    "\n",
    "    results = [('r-squared:',r2),\n",
    "               ('mean absolute error:',mae),\n",
    "               ('mean squared error:',mse), \n",
    "               ('root Mean squared error:',rmse)]\n",
    "\n",
    "    print('\\n' + 'model metrics:' + '\\n')\n",
    "    for label, value in results:\n",
    "        print(f\"{label:{35}} {value:.>{20}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation regression metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation regression metrics\n",
    "def regression_cross_val(regressor, X_train, y_train, cv):\n",
    "    \n",
    "    # r2\n",
    "    ###############################################################################################################\n",
    "    accuracies_r2 = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = cv, n_jobs = -1, \n",
    "                                    scoring = 'r2')\n",
    "    accuracies_r2_mean = round(accuracies_r2.mean(),2)\n",
    "    accuracies_r2_std = round(accuracies_r2.std(),2)\n",
    "    \n",
    "    # MAE\n",
    "    ###############################################################################################################\n",
    "    accuracies_mae = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = cv, n_jobs = -1, \n",
    "                                     scoring = 'neg_mean_absolute_error')\n",
    "    \n",
    "    score_mae = [score * -1 for score in accuracies_mae]\n",
    "    score_mae_df = pd.DataFrame(score_mae, columns = ['col'])\n",
    "    accuracies_mae_mean = round(score_mae_df['col'].mean(),2)\n",
    "    accuracies_mae_std = round(score_mae_df['col'].std(),2)\n",
    "    \n",
    "    # MSE\n",
    "    ###############################################################################################################\n",
    "    accuracies_mse = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = cv, n_jobs = -1, \n",
    "                                     scoring = 'neg_mean_squared_error')\n",
    "    \n",
    "    score_mse = [score * -1 for score in accuracies_mse]\n",
    "    score_mse_df = pd.DataFrame(score_mse, columns = ['col'])\n",
    "    accuracies_mse_mean = round(score_mse_df['col'].mean(),2)\n",
    "    accuracies_mse_std = round(score_mse_df['col'].std(),2)\n",
    "    \n",
    "    # RMSE\n",
    "    ###############################################################################################################\n",
    "    accuracies_rmse = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10, n_jobs = -1, \n",
    "                                      scoring = 'neg_root_mean_squared_error')\n",
    "    \n",
    "    score_rmse = [score * -1 for score in accuracies_rmse]\n",
    "    score_rmse_df = pd.DataFrame(score_rmse, columns = ['col'])\n",
    "    accuracies_rmse_mean = round(score_rmse_df['col'].mean(),2)\n",
    "    accuracies_rmse_std = round(score_rmse_df['col'].std(),2)\n",
    "    \n",
    "    # Tuple unpacking\n",
    "    ###############################################################################################################\n",
    "    \n",
    "    print('\\n' + 'test model f-fold metrics:' + '\\n')\n",
    "        \n",
    "    plt.figure(figsize = (10,3))\n",
    "    plt.plot(range(1, cv + 1, 1), accuracies_r2, ls = '-', marker = 'o')\n",
    "    plt.title('r2 for kfold')\n",
    "    plt.xlabel('kfold index')\n",
    "    plt.ylabel('r2')\n",
    "    plt.ylim(0,1.1)\n",
    "    plt.show()\n",
    "    \n",
    "    results1 = [(\"r-squared k-fold cross validation: \", accuracies_r2_mean),\n",
    "                (\"r-squared std: \", accuracies_r2_std)]\n",
    "\n",
    "    for label, value in results1:\n",
    "        print(f\"{label:{50}} {value:.>{20}}\")\n",
    "    \n",
    "    plt.figure(figsize = (10,3))\n",
    "    plt.plot(range(1, cv + 1, 1), score_mae, ls = '-', marker = 'o')\n",
    "    plt.title('mae for kfold')\n",
    "    plt.xlabel('kfold index')\n",
    "    plt.ylabel('mae')\n",
    "    plt.ylim(0, max(score_mae) * 1.25)\n",
    "    plt.show()\n",
    "    \n",
    "    results2 = [(\"mean absolute error k-fold cross validation: \", accuracies_mae_mean),\n",
    "                (\"mean absolute error std: \", accuracies_mae_std)]\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    for label, value in results2:\n",
    "        print(f\"{label:{50}} {value:.>{20}}\")\n",
    "        \n",
    "    plt.figure(figsize = (10,3))\n",
    "    plt.plot(range(1, cv + 1, 1), score_mse, ls = '-', marker = 'o')\n",
    "    plt.title('mse for kfold')\n",
    "    plt.xlabel('kfold index')\n",
    "    plt.ylabel('mse')\n",
    "    plt.ylim(0, max(score_mse) * 1.25)\n",
    "    plt.show()\n",
    "    \n",
    "    results3 = [(\"mean squared error k-fold cross validation: \", accuracies_mse_mean),\n",
    "                (\"mean squared error std: \", accuracies_mse_std)]\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    for label, value in results3:\n",
    "        print(f\"{label:{50}} {value:.>{20}}\")\n",
    "        \n",
    "    plt.figure(figsize = (10,3))\n",
    "    plt.plot(range(1, cv + 1, 1), score_rmse, ls = '-', marker = 'o')\n",
    "    plt.title('rmse for kfold')\n",
    "    plt.xlabel('kfold index')\n",
    "    plt.ylabel('rmse')\n",
    "    plt.ylim(0, max(score_rmse) * 1.25)\n",
    "    plt.show()\n",
    "    \n",
    "    results4 = [(\"root mean squared error k-fold cross validation: \", accuracies_rmse_mean),\n",
    "                (\"root mean squared error std: \", accuracies_rmse_std)]\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    for label, value in results4:\n",
    "        print(f\"{label:{50}} {value:.>{20}}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Bar Chart and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance bar chart followed by supporting stats\n",
    "def regression_feature_importance(model, X_cols, font, length, width):\n",
    "    \n",
    "    coefficients = model.coef_\n",
    "    intercept =  np.array([model.intercept_])\n",
    "    coefficients = np.concatenate([coefficients, intercept])\n",
    "    coefficients = coefficients.reshape((-1, 1))\n",
    "    X_col = np.array(X_cols.columns)\n",
    "    X_col = np.concatenate([X_col, np.array(['intercept'])], axis = 0)\n",
    "    X_col = X_col.reshape((-1, 1))\n",
    "    coefficients = np.concatenate((X_col, coefficients), axis = 1)\n",
    "    coefficients = pd.DataFrame(coefficients, columns = ['', 'coef'])\n",
    "    coefficients['positive'] = coefficients['coef'] > 0 \n",
    "    coefficients['coef2'] = abs(coefficients['coef'])\n",
    "    coefficients = coefficients.sort_values(by = ['coef2'], ascending = True)\n",
    "\n",
    "    sns.set(font_scale = font, style = 'white')  \n",
    "    coefficients['coef'].plot(kind = 'barh',\n",
    "                              figsize = (width, length), \n",
    "                              color = coefficients.positive.map({True:'b', False:'r'}))\n",
    "    plt.title('feature importance (features scaled)')\n",
    "    plt.xlabel('coefficient units')\n",
    "    plt.ylabel('features')\n",
    "    plt.show()\n",
    "\n",
    "    coefficients = coefficients.sort_values(by = ['coef2'], ascending = False)\n",
    "    coefficients = coefficients.drop(['coef2'], axis = 1, errors = 'ignore')\n",
    "    coefficients = coefficients.set_index('')\n",
    "\n",
    "    display(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Means and Counts Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual means and counts plot\n",
    "def residual_means_counts_plot(df, X, res, ymin1, ymax1, ymin2, ymax2):\n",
    "    col_range = round(df[X].max() - df[X].min(),0) + 1\n",
    "    bins = pd.cut(df[X], int(col_range))\n",
    "    mean_res = df.groupby(bins).agg({res: \"mean\"})\n",
    "    mean_res = mean_res.rename(columns = {res: 'mean'}, inplace = False).reset_index()\n",
    "    count_res = df.groupby(bins).agg({res: \"count\"})\n",
    "    count_res = count_res.rename(columns = {res: 'count'}, inplace = False).reset_index()\n",
    "    mean_count_res = pd.merge(mean_res, count_res, on = X)\n",
    "    mean_count_res[X] = mean_count_res[X].astype(str)\n",
    "    \n",
    "    # Plot the results\n",
    "    sns.set(font_scale = 1, style = 'white') \n",
    "    plt.figure(figsize = (15,3))\n",
    "    ax1 = plt.axes()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(mean_count_res[X], mean_count_res['mean'], ls = '-', marker = 'o')\n",
    "    ax1.set_xticklabels(mean_count_res[X], rotation = 'vertical')\n",
    "    ax1.axhline(y = 0, color = 'r', linestyle = '--')\n",
    "    ax1.set(xlabel = X + ' bins', ylabel = 'average residuals per bin', title = 'average residuals per binned ' + X)\n",
    "    ax1.set_ylim(mean_count_res['mean'].min() + ymin1, mean_count_res['mean'].max() + ymax1)\n",
    "\n",
    "    ax2.bar(mean_count_res[X],  mean_count_res['count'], color = 'darkgreen')\n",
    "    ax2.set(ylabel = 'count per bin')\n",
    "    ax2.set_ylim(mean_count_res['count'].min() + ymin2, mean_count_res['count'].max() + ymax2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "387px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
