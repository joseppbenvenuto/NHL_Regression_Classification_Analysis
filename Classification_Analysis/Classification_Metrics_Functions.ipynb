{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Functions\n",
    "\n",
    "### Description\n",
    "\n",
    "Below are the different functions used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math as ma\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad Classification Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates all classification metrics \n",
    "def cap_auc(model, df, target, y, y_pred, y_score, X, length, width):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    # Concordance and Discordance\n",
    "    Probability = model.predict_proba(X)\n",
    "    Probability1 = pd.DataFrame(Probability)\n",
    "    Probability1.columns = ['Prob_0','Prob_1']\n",
    "    TruthTable = pd.merge(y[[target]], Probability1[['Prob_1']], how = 'inner', \n",
    "                          left_index = True, right_index = True)\n",
    "    zeros = TruthTable[(TruthTable[target] == 0)].reset_index(drop = True)\n",
    "    ones = TruthTable[(TruthTable[target] == 1)].reset_index(drop = True)\n",
    "    \n",
    "    from bisect import bisect_left, bisect_right\n",
    "    zeros_list = sorted([zeros.iloc[j,1] for j in zeros.index])\n",
    "    zeros_length = len(zeros_list)\n",
    "    disc = 0\n",
    "    ties = 0\n",
    "    conc = 0\n",
    "    for i in ones.index:\n",
    "        cur_conc = bisect_left(zeros_list, ones.iloc[i,1])\n",
    "        cur_ties = bisect_right(zeros_list, ones.iloc[i,1]) - cur_conc\n",
    "        conc += cur_conc\n",
    "        ties += cur_ties\n",
    "        \n",
    "    pairs_tested = zeros_length * len(ones.index)\n",
    "    disc = pairs_tested - conc - ties\n",
    "    concordance = round(conc/pairs_tested,2)\n",
    "    discordance = round(disc/pairs_tested,2)\n",
    "    ties_perc = round(ties/pairs_tested,2)\n",
    "    Somers_D = round((conc - disc)/pairs_tested,2)\n",
    "    \n",
    "    results1 = [('Pairs: ', pairs_tested),\n",
    "                ('Conc: ', conc),\n",
    "                ('Disc: ', disc),\n",
    "                ('Tied: ', ties)]\n",
    "    \n",
    "    print('\\n')\n",
    "    for label, value in results1:\n",
    "        print(f\"{label:{35}} {value:.>{20}}\")\n",
    "        \n",
    "    results2 = [('Concordance: ', concordance),\n",
    "                ('Discordance: ', discordance),\n",
    "                ('Tied: ', ties_perc),\n",
    "                ('Somers D: ', Somers_D)]\n",
    "    \n",
    "    print('\\n')\n",
    "    for label, value in results2:\n",
    "        print(f\"{label:{35}} {value:.>{20}}\")\n",
    "    \n",
    "    # ROC plot\n",
    "    probs = y_score\n",
    "    fpr, tpr, thresholds = roc_curve(y, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print('\\n')\n",
    "    plt.figure(figsize = (width, length))\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    label = 'classifier:' + ' {0:.2f}'.format(roc_auc)\n",
    "    plt.plot(fpr, tpr, c = 'g', label = label, linewidth = 2)\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.title('receiver operating characteristic')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    auc = round(roc_auc_score(y, y_score),2)\n",
    "\n",
    "    results = [('AUC:',auc)]\n",
    "    \n",
    "    print('\\n')\n",
    "    for label, value in results:\n",
    "        print(f\"{label:{35}} {value:.>{20}}\")\n",
    "        \n",
    "    # General Classification metrics\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    tp = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "\n",
    "    tn = cm[1][1]\n",
    "    fn = cm[1][0]\n",
    "    \n",
    "    print('\\n')\n",
    "    cm_df = pd.DataFrame([{'1': tp, '0': fp}, {'1': fn, '0': tn}])\n",
    "    cm_df = cm_df.set_index([pd.Index([1,0])])\n",
    "    print('confussion matrix' + '\\n')\n",
    "    print(cm_df)\n",
    "\n",
    "    accuracy = round((tp + tn) / (tp + fp + tn + fn), 2)\n",
    "    precision = round((tp) / (tp + fp),2)\n",
    "    recall = round((tp) / (tp + fn),2)\n",
    "    f1 = round((2 * (precision * recall)) / (precision + recall),2)\n",
    "\n",
    "    results3 = [('Accuracy:', accuracy),\n",
    "                ('Precision:', precision),\n",
    "                ('Recall:', recall),\n",
    "                ('F1:', f1)]\n",
    "    \n",
    "    print('\\n')\n",
    "    for label, value in results3:\n",
    "        print(f\"{label:{35}} {value:.>{20}}\")\n",
    "\n",
    "    # Cap plot\n",
    "    y = y.to_numpy()\n",
    "    y_pred = y_pred.astype(int).to_numpy()\n",
    "    y_score = y_score.to_numpy()\n",
    "\n",
    "    total = len(y)\n",
    "    class_1_count = np.sum(y)\n",
    "    class_0_count = total - class_1_count\n",
    "\n",
    "    probs = y_score\n",
    "    model_y = [y for _, y in sorted(zip(probs, y), reverse = True)]\n",
    "    y_values = np.append([0], np.cumsum(model_y))\n",
    "    X_values = np.arange(0, total + 1)\n",
    "\n",
    "    print('\\n')\n",
    "    sns.set(font_scale = 1, style = 'white')\n",
    "    plt.figure(figsize = (width, length))\n",
    "    plt.plot([0, total], [0, class_1_count], c = 'r', linestyle = '--', label = 'random model')\n",
    "\n",
    "    plt.plot([0, class_1_count, total],[0, class_1_count, class_1_count], c = 'grey', \n",
    "             linewidth = 2, label = 'perfect model')\n",
    "\n",
    "    plt.plot(X_values, y_values, c = 'b', label = 'classifier', linewidth = 2)\n",
    "\n",
    "    plt.xlabel('total observations')\n",
    "    plt.ylabel('class 1 observations')\n",
    "    plt.title('cumulative accuracy profile')\n",
    "    plt.legend(loc = 'lower right')\n",
    "\n",
    "    index = int((50*total / 100))\n",
    "\n",
    "    plt.plot([index, index], [0, y_values[index]], c ='g', linestyle = '--')\n",
    "\n",
    "    plt.plot([0, index], [y_values[index], y_values[index]], c = 'g', linestyle = '--')\n",
    "\n",
    "    class_1_observed = y_values[index] * 100 / max(y_values)\n",
    "    plt.show()\n",
    "    \n",
    "    # Cap table\n",
    "    rows_decile = round(len(df) / 10, 0)\n",
    "    flag_count = df[target].sum()\n",
    "    cap_table = df\n",
    "    cap_table = cap_table.sort_values(by = 'predicted_proba', ascending = False).reset_index(drop = True)\n",
    "    cap_table['count'] = 1\n",
    "    cap_table['count_of_rows'] = 1\n",
    "    cap_table['count'] = cap_table['count'].cumsum()\n",
    "    cap_table['bin'] = np.ceil(cap_table['count'] / rows_decile)\n",
    "    cap_table['bin'][cap_table['bin'] > 10] = 10\n",
    "    cap_table = cap_table.groupby(by = ['bin']).sum().reset_index()\n",
    "    cap_table = cap_table[['bin', 'count_of_rows', target]]\n",
    "    cap_table['model_percent'] = round((cap_table[target] / flag_count) * 100, 2)\n",
    "    cap_table['random_percent'] = 10\n",
    "    cap_table['model_cumm_percent'] = cap_table['model_percent'].cumsum()\n",
    "    cap_table['random_cumm_percent'] = cap_table['random_percent'].cumsum()\n",
    "    cap_table['ks'] = cap_table['model_cumm_percent'] - cap_table['random_cumm_percent']\n",
    "    cap_table.loc[len(cap_table)] = 0\n",
    "    cap_table = cap_table.sort_values(by = 'bin', ascending = True).reset_index(drop = True)\n",
    "    return cap_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Summary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides a summary of the logistic regression\n",
    "def logit_summary(X, y):\n",
    "    X2 = sm.add_constant(X)\n",
    "    logit_model = sm.Logit(y, X2)\n",
    "    result = logit_model.fit()\n",
    "    print(result.summary2())"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
